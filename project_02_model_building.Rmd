---
title: "project_02_models"
author: "Ryan Folks"
date: "8/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(MASS)
library(tidyverse)
library(gridExtra)
library(corrplot)
library(ROCR)
```

```{r}
white_wine <- read.csv('wineQualityWhites.csv')
red_wine <- read.csv('wineQualityReds.csv')

white_wine <- dplyr::select(white_wine, -c(free.sulfur.dioxide, X))
red_wine <- dplyr::select(red_wine, -c(free.sulfur.dioxide, X))

red_wine <- red_wine %>%
  mutate(color = 0) #red is 0
white_wine <- white_wine %>%
  mutate(color = 1) #white is 1

#add quality_cat
red_wine <- red_wine %>%
  mutate(quality_cat = ifelse(quality>=7, 1, 0)) #high quality 1, low quality 0
white_wine <- white_wine %>%
  mutate(quality_cat = ifelse(quality>=7, 1, 0)) #high quality 1, low quality 0

#change color to factor
red_wine$color<-factor(red_wine$color)
levels(red_wine$color) <- c('red') 
white_wine$color<-factor(white_wine$color)
levels(white_wine$color) <- c('white')

#change quality_cat to factor 
red_wine$quality_cat<-factor(red_wine$quality_cat)
levels(red_wine$quality_cat) <- c("low","high") 
white_wine$quality_cat<-factor(white_wine$quality_cat)
levels(white_wine$quality_cat) <- c("low","high") 

wine_combined <- rbind(white_wine, red_wine) #combine datasets 

#split into train and test data
set.seed(1) #set seed 
#white
sample_white<-sample.int(nrow(white_wine), floor(.80*nrow(white_wine)), replace = F)
train_white<-white_wine[sample_white, ] #training data 
test_white<-white_wine[-sample_white, ] #testing data

#red
sample_red<-sample.int(nrow(red_wine), floor(.80*nrow(red_wine)), replace = F)
train_red<-red_wine[sample_red, ] #training data 
test_red<-red_wine[-sample_red, ] 

#combined
train_combined <- rbind(train_white, train_red)
test_combined <- rbind(test_white, test_red) 
```


# Model Building

## Red Wine Model
```{r}
train_red <- train_red %>% dplyr::select(-c(color, quality))
train_white <- train_white %>% dplyr::select(-c(color, quality))
```

Our initial Exploratory Data Analysis suggests that we should build two separate models to classify the quality of red and white wines as "low" or "high". We decided to employ the all possible regressions procedure to determine which of the ten predictors may be considered the most important in predicting the quality category of red wine. All-possible-regressions is an automated procedure that goes beyond stepwise regression and tests all possible subsets of the set of potential independent variables, which in the context of our problem is 2^10 = 1024 possible subsets. We set the default value for nbest to 1, which tells the algorithm to return the one best set of predictors (based on R2) for each number of possible predictors. We also set nvmax to 10 so all possible subsets are displayed.

```{r}
allreg_red <- regsubsets(quality_cat ~., data=train_red, nbest=1, nvmax=10)
summary(allreg_red)
#R2
which.max(summary(allreg_red)$adjr2)
#model 9 is the best based on R2
#CP
which.min(summary(allreg_red)$cp)
#model 8 is the best based on Mallow's CP
#BIC
which.min(summary(allreg_red)$bic)
#model 7 based on BIC
```

After running the algorithm, we extract the best models based on the following criteria: R2, Mallow's Cp and BIC. For allreg_red, model 9 has the best adjusted R2, model 8 has the best Mallow's Cp and model 7 has the best BIC. In our case, the model number corresponds to the number of predictors being used. Based on these results, we select the proposed 7-predictor model for our initial model building process. The 7-predictor model uses the following predictor variables to classify the quality of red wine as "low" or "high": alcohol, volatile.acidity, sulphates, fixed.acidity, density, residual.sugar and chlorides.

```{r}
#all possible regression model, based on BIC -> 7-predictor model
model1_red <- glm(quality_cat~ alcohol+volatile.acidity+sulphates+fixed.acidity+density+residual.sugar+chlorides, 
                  family='binomial', data=train_red)
summary(model1_red) #AIC score 738.12
```
The fitted 7-predictor logistic regression equation for red wine is displayed as follows:

log(pi(quality)/1+pi(quality)) = 285.10221 + (0.72351)alcohol + (-2.82651)volatile.acidity + (3.23950)sulphates +  (0.36775)fixed.acidity + (-299.78128)density + (0.17416)residual.sugar + (-7.54886)chlorides  

The Z test associated with all the coefficients are considered statistically significant at the .05 level, which suggests that all 7 predictor variables should be kept in our model. 

The next step in our procedure is to check that our proposed model meets all the assumptions for logistic regressions, which are defined as follows:

1) There is a linear relationship between the logit of the outcome and each predictor variables. 
2) There are no influential values (extreme values or outliers) in the continuous predictors
3) There are no high intercorrelations (i.e. multicollinearity) among the predictors.

To determine that the first assumption has been met, we check the linear relationship between continuous predictor variables and the logit of the outcome. This is done by visually inspecting the scatter plot between each predictor variable and the logit values. 
```{r}
probabilities <- predict(model1_red, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
head(predicted.classes)

model_red <- train_red%>%
  dplyr::select(is.numeric) 
predictors <- colnames(model_red)
#Bind the logit and tidying the data for plot
model_red <- model_red %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)

ggplot(model_red, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")
```
The smoothed scatter plots show that alcohol, sulphates and volatile.acidity are quite linearly associated with the quality_cat outcome in logit scale. The smoother scatter plots for the other predictor variables show little to no linear association with the quality_cat outcome in logit scale, which leads us to believe they are weaker predictors. None of the scatterplots lead us to believe that assumption 1 has been violated, and we therefore move on without transforming variables and begin to explore our next assumption. 

Next, we generate the residuals, studentized residuals and externally studentized residuals associated with our 7-predictor model to help detect any outliers. We combine these residual values into a dataframe and create scatterplots of the studentized and externally studentized residuals in order to visually inspect for the presence of outliers.
```{r}
#ASSUMPTION 2 -> outliers and inluential variables 
#residuals -> used to help detect outliers 
res<-model1_red$residuals
# studentized residuals 
student.res<-rstandard(model1_red)
# externally studentized residuals
ext.student.res<-rstudent(model1_red)

res.frame<-data.frame(res,student.res,ext.student.res)
res.frame

par(mfrow=c(1,2))
plot(model1_red$fitted.values,student.res,
     main="Studentized Residuals",
     ylim=c(-4.5,4.5))
plot(model1_red$fitted.values,ext.student.res,
     main="Externally Studentized Residuals",
     ylim=c(-4.5,4.5))
#plots look very similar, suggests that there are no outliers 
```
The resulting plots look very similar, which leads us to believe there are no outliers in our model. 

Next, we calculate leverages to identify how far each observation is from the center of the predictor space. If the leverage is greater than 2p/n, the associated observation may be deemed to have high leverage and is considered outlying in the predictor space. In this case p, which corresponds to the number of predictor variables in our model, is 7 and n, the number of records in our training set for red wine, is 1279. 
```{r}
#leverage
p <- 7 #7 predictor model
n<-dim(train_red)[1]
lev<-lm.influence(model1_red)$hat 
##identify high leverage points 
lev[lev>2*p/n]
#high leverage observations are data points that are likely to be influential 
```

Due to the number of observations that were found to have high leverage, we decided to calculate Cook's Distance in order to determine if these high leverage observations were influential and were consequently negatively affecting our regression model. We decided to use Cook's distance measure as we are mainly concerned with the predictive ability of our logistic model. A large value indicates that the observation has a large influence on the results. The Cook's Distance of each observation was compared to the a cut off value generated by the following F distribution: F0.5,p,n-p, where p is 7 and n is 1279.

```{r}
#influential 
#cooks distance
COOKS<-cooks.distance(model1_red)
COOKS[COOKS>qf(0.5,p,n-p)]
#no influential points based on COOK's distance

#DONT THINK WE NEED THESE
##dffits
DFFITS<-dffits(model1_red)
DFFITS[abs(DFFITS)>2*sqrt(p/n)]
#shows influential points, but we are concerned with prediction, rely more heavily on cook's distance

#dfbetas
DFBETAS<-dfbetas(model1_red)
abs(DFBETAS)>2/sqrt(n)
#shows influential points, again more concerned with prediction, rely more on cook's distance 
```

None of the generated Cook's distance values exceeded the cutoff value, which leads us to believe that none of our observations in our model are influential. We conclude that assumption 2 has been met, and move on to the final assumption.

Next, we check for multicollinearity in our model. Multicollinearity corresponds to a situation where two or more predictors are linearly dependent on each other. When two or more predictors are linearly dependent, they do not provide independent information to the response, resulting in large standard errors for the coefficients. We can check for multicollinearity by computing the variance inflation factors (VIFs) for each predictor variable. As a rule of thumb, A VIF value that exceeds 5 indicates a problematic amount of collinearity and needs to be explored, whereas a value greater than 10 indicates a serious amount of collinearity and must be immediately addressed. 

```{r}
#ASSUMPTION 3
#mutlicollinearity 
vif(dplyr::select(train_red, c(alcohol,volatile.acidity,sulphates,fixed.acidity,density,residual.sugar,chlorides)))
#VIF score above 10 indicates high correlation, cause for concern, VIF scores less than 10 across the board
#############################################################
```
The resulting VIFs are below 5 for all 7 of the predictor variables in our model, which leads us to conclude that the third assumption has been satisfied. 

After checking that all the model assumptions have been met, we move on to comparing our 7-predictor model with other possible logistic regression models using Likelihood Ratio Tests (LRTs) to evaluate if our proposed model is useful in classifying red wine as either "low" or "high" quality (quality_cat). 

First, we compare our 7-predictor model to the intercept only model. 

The null hypothesis is:

Ho:beta(alcohol)=beta(volatile.acidity)=beta3(sulphates)=beta4(fixed.acidity)=beta5(density)=beta6(residual.sugar)=
beta7(chlorides)=0 

versus the alternative hypothesis:

Ha:at least one of the coefficients is not zero. 

We calculate the test statistic, which measures the difference in loglikelihoods of the models we are comparing, and compare it with a chi square distribution, where the degrees of freedom equal the number of coefficients we are testing (7). For this comparison, the test statistic is the difference in the null deviance and residual deviance of the 7-predictor model.
```{r}
#TESTING MODEL 7-PRED MODEL
#############################################################
#Test that 7-predictor model is useful
##test if coefficients for all 7 predictors are 0 
#H0: beta1=beta2=beta3=beta4=beta5=beta6=beta7=0
#HA: at least one of the coefficients in HO is not zero
##test stat
TS<-model1_red$null.deviance-model1_red$deviance
TS
#p-value 
1-pchisq(TS,7)
#p-value is 0
#So we reject the null hypothesis. The 7-predictor model is chosen over the intercept-only model;
#the 7-predictor model is useful.
```
The generated test statistic is 291.5332 with an associated p-value of 0. Therefore, we reject the null hypothesis and conclude that our 7-predictor model is useful and should be selected over the intercept-only model.

Next, we decide to compare our 7-predictor model to the full (10-predictor) model. 

The null hypothesis is:

Ho:beta(pH)=beta(citric.acid)=beta(total.sulfur.dioxide)=0

versus the alternative hypothesis: 

Ha:at least one of the coefficients is not zero. 

Again we calculate the test statistic, and compare it with a chi square distribution with 3 degrees of freedom (as we are testing 3 coefficients). For this comparison, the test statistic is the difference in the residual deviances of both models.
```{r}
#test if 7-predictor model more useful then full (10 predictor model)
#H0: beta8=beta9=beta10=0
#HA: at least one of the coefficients in HO is not zero
full_red<-glm(quality_cat ~ ., family = "binomial", data=train_red)

TS2<-model1_red$deviance-full_red$deviance
TS2  #13.3604
#pvalue
1-pchisq(TS2,3) #0.003918644
```
The generated test statistic is 13.3604 with an associated p-value of 0.003918644. Therefore, we reject the null hypothesis and conclude that the full (10-predictor) model is more useful than the 7-predictor model.

We found this result to be interesting, and decided to explore the full model in more depth.

```{r}
summary(full_red)
```
We see that the Z test scores associated with the coefficients for predictors pH and citric.acid are insignificant. The next step is to compute VIFs for each of the 10 predictors in the full model to determine if multicollinearity exists between any of the variables.

```{r}
#check full model multicollinearity 
vif(dplyr::select(train_red, c(alcohol,volatile.acidity,sulphates,fixed.acidity,density,residual.sugar,chlorides, pH,
                               citric.acid, total.sulfur.dioxide)))
#no multicollinearity, pH and citric.acid appear to not be significant predictors (based on 
#wald test scores), can drop, end up with 8-predictor model
```
The resulting VIFs suggests that none of the variables are linearly dependent on one another, and we conclude that pH and citric.acid are not significant predictors for classifying red wine as "low" or "high" an drop them from the full model. 

To validate dropping both of these predictors from the full model, we compare our 8-predictor model to the full (10-predictor) model. 

The null hypothesis is:

Ho:beta(pH)=beta(citric.acid)=0

versus the alternative hypothesis: 

Ha: at least one of the coefficients is not zero. 

Again we calculate the test statistic, and compare it with a chi square distribution with 2 degrees of freedom (as we are testing 2 coefficients). 
```{r}
#8-predictor model
model2_red <- glm(quality_cat~alcohol+volatile.acidity+sulphates+fixed.acidity+density+residual.sugar+chlorides+
                  total.sulfur.dioxide, family = "binomial", data=train_red)
summary(model2_red)

#H0: beta(pH)=beta(citric.acid)
#Ha: at least one of the coefficients in H0 is non-zero

TS3<-model2_red$deviance-full_red$deviance
TS3
1-pchisq(TS3,2) #p-value: 0.5319783
```
The generated test statistic is 1.262305 with an associated p-value of 0.5319783. Therefore, we fail to reject the null hypothesis and select the 8-predictor model over the full model.

The fitted 8-predictor logistic regression equation for red wine is displayed as follows:

log(pi(quality)/1+pi(quality)) = 316.5 + (0.6657)alcohol + (-2.720)volatile.acidity + (3.739)sulphates +  (0.3373)fixed.acidity + (-330.4)density + (0.2517)residual.sugar + (-8.472)chlorides + (-0.01188)total.sulfur.dioxide

We now have two candidate models for classifying the quality of red wine: our original 7-predictor model (model1_red) and the 8-predictor model (model2_red). We decide to test both these models using the test_red dataset in order to determine which is more useful in answering our research question. 

First, we store the predicted probabilities of the test_red data based on our 7-predictor logistic regression model in the variable preds. We then transform this data using the prediction() function and store the values of the true positive rate and false positive rate in the roc_result object. We then use this object to generate a ROC curve for our 7-predictor logistic regression model. The ROC curve plots the true positive rate (TPR) against the false positive rate (FPR) of the model as the threshold value is varied from 0 to 1. The ROC curve generated by our 7-predictor model lies above the diagonal, which indicates the logistic regression does a better job than random guessing when classifying red wines as "low" or "high" quality. We also generate an Area Under the Curve (AUC) associated with this ROC curve. The AUC value for our model is 0.889574, which again provides evidence that our model does better at classifying red wines then random guessing (represented by an AUC score of 0.5). Finally, we produce a confusion matrix, which contains the number of observations in each class ("low" and "high" quality red wine) and the number of predicted observations in each class (for test_red) generated by our 7-predictor model using a threshold value of 0.5. 
```{r}
#ROC/AUC/CONFUSION MATRIX TO COMPARE 7 AND 8-PRED MODELS 
#####################################################
##ROC AND AUC FOR 7-PREDICTOR MODEL##
##predicted survival rate for test data based on training data
preds<-predict(model1_red, newdata=test_red, type="response")
##transform the input data into a format that is suited for the
##performance() function
rates<-prediction(preds, test_red$quality_cat, label.ordering = c("low", "high"))
##store the true positive and false positive rates
roc_result<-performance(rates,measure="tpr", x.measure="fpr")

##plot ROC curve and overlay the diagonal line for random guessing
plot(roc_result, main="ROC Curve for Red Wine Quality Prediction")
lines(x = c(0,1), y = c(0,1), col="red")

auc<-performance(rates, measure = "auc")
auc@y.values #0.8896574, suggests are model classify observations better than random guessing 

table(test_red$quality_cat, preds>0.50)
#####################################################
```

Next, we follow a similar assessment procedure using the test_red dataset and the 8-predictor model.The ROC curve generated by our 8-predictor model lies above the diagonal, which indicates the logistic regression does a better job than random guessing when classifying red wines as "low" or "high" quality.  The AUC value for this model is 0.8978096, which again provides evidence that our 8-predictor model does slightly better at classifying red wines than our 7-predictor model. Finally, we produce a confusion matrix generated by the 8-predictor model using a threshold value of 0.5. 
```{r}
#####################################################
##ROC AND AUC FOR 8-predictor MODEL##
preds_2<-predict(model2_red, newdata=test_red, type="response")
##transform the input data into a format that is suited for the
##performance() function
rates_2<-prediction(preds_2, test_red$quality_cat, label.ordering = c("low", "high"))
##store the true positive and false positive rates
roc_result_2<-performance(rates_2,measure="tpr", x.measure="fpr")

##plot ROC curve and overlay the diagonal line for random guessing
plot(roc_result_2, main="ROC Curve for Red Wine Quality Prediction")
lines(x = c(0,1), y = c(0,1), col="red")

auc_full<-performance(rates_2, measure = "auc")
auc_full@y.values #0.8978096, suggests are model classify observations better than random guessing 

table(test_red$quality_cat, preds_2>0.50)
#####################################################
```
In terms of our research question, we are most interested in maximizing the ratio of the true positive results to the false positive results. The number of false positive results (low quality wines incorrectly categorized as high quality) is 6 and the number of true positive results (high quality wines correctly categorized as high quality) is 15 for our 7-predictor model, whereas the number of false positive results is 9 and the number of true positive results is 15 for our 8-predictor model. The calculated ratio of true positive results to false positive results is larger for our 7-predictor model, so we select it over the 8-predictor model. 























## White Wine Model  


























### Overview  

Our goal, like with red wines, is to select a model which maximizes the ratio of true positives to false positives.  
The model selection process for white wine was by no means straightforward. We began with a 7 predictor model, then moved to the full model, and finally settled upon a 5 predictor model which both outperformed either of the previous models, contained fewer predictors, and contained no serious multicolinearity.  

60 percent of the wines selected by this final model are truly high quality, which is far higher than the 22 percent that random guessing selects.

### Searching all First Order Models

We begin by seeing what the BIC, Mallows $C_p$, and adjusted $R^{2}$ select as their best model from all possible models.  

```{r}
allreg_white <- regsubsets(quality_cat ~., data=train_white, nbest=1, nvmax=10)
summary(allreg_white)
```

```{r}
which.min(summary(allreg_white)$bic)
which.min(summary(allreg_white)$cp)
which.max(summary(allreg_white)$adjr2)
```

All the criteria have selected the 7th model as their best, so we will fit that model as our initial model and assess it.  

```{r}
model7_white <- glm(quality_cat ~ fixed.acidity + volatile.acidity + residual.sugar + density + pH + sulphates + alcohol,
                    family='binomial', data=train_white)
summary(model7_white)
```

Alcohol fails the wald test. This is counter intuitive, since alcohol has the highest correlation with quality.  
Alcohol in white wines is also highly correlated with both density and residual sugar, so there is likely multicolinearity present in the model. Before addressing this, we should test whether this model is even better than the intercept, and then test if it is better than the model with all predictors.  

### Is the 7 predictor model better than the intercept only model?  

Our hypothesis here are $H_{0} : \beta_{1} = \beta_{2} = \ldots = \beta_{7}, H_{a} : \exists x \in \{1, 2, \ldots , 7\} s.t. \beta_{x} \neq 0$  

```{r}
TS<-model7_white$null.deviance-model7_white$deviance
TS
#p-value 
1-pchisq(TS,6)
```

The code has produced a chi squared test statistic of 757.2668, and a corresponding p-value of $\approx 0$.  
Therefore we reject $H_{0}$ and conclude that at least one of our predictors is linearly related to the log odds of our quality category variable.  

### Is the 10 predictor model better than the 7 predictor model?  

Our hypothesis are $H_{0} : \beta_{8} = \beta_{9} = \beta_{10}, H_{a} : \exists x \in \{8, 9, 10\} s.t. \beta_{x} \neq 0$  

```{r}
full_white<-glm(quality_cat ~ ., family = "binomial", data=train_white)
summary(full_white)

TS2<-model7_white$deviance-full_white$deviance
TS2  #1.502435
#pvalue
1-pchisq(TS2,3)
```

We get a chi squared test statistic of 15.15094 and a corresponding p-value of $\approx 0.001$.  
Therefore, we reject the null hypothesis and conclude that at least one predictor is significant.  
This means that we prefer the full model over the six predictor model, and will add in the rest of the predictors.  

The full model has wald tests that fail. Many of the predictors are highly correlated with one another, so we must address multicolinearity.  

### Multicollinearity  

Before moving forward, we should check to see if unacceptable levels of multicolinearity are in the full model.  

```{r}
faraway::vif(dplyr::select(train_white, -quality_cat))
```

```{r}
model8_white <- glm(quality_cat ~ fixed.acidity + volatile.acidity + citric.acid + chlorides + total.sulfur.dioxide + pH + sulphates                                 + alcohol,
                    family='binomial', data=train_white)
summary(model8_white)
```

Both residual sugar and density have a VIF higher than 10, so we should take them out of the model.  

```{r}
faraway::vif(dplyr::select(train_white, c(fixed.acidity, volatile.acidity, citric.acid, chlorides, total.sulfur.dioxide, pH, sulphates, alcohol)))
```

While we no longer have any multicolinearity, fixed acidity, citric acid, and total sulpher dioxide fail their wald test, so we need to test if we can remove them from the model as well.  

```{r}
model5_white <- glm(quality_cat ~ volatile.acidity + chlorides + pH + sulphates + alcohol,
                    family='binomial', data=train_white)
summary(model5_white)
```

```{r echo=FALSE, results='hide'}
full_white<-glm(quality_cat ~ ., family = "binomial", data=train_white)
summary(full_white)

TS2<-model5_white$deviance-model8_white$deviance
TS2  #1.502435
#pvalue
1-pchisq(TS2, 3)
```

Our test fails to reject, so we may remove these three variables and continue with the 5 predictor model.  

```{r}
##predicted survival rate for test data based on training data
preds<-predict(model5_white, newdata=test_white, type="response")
##transform the input data into a format that is suited for the
##performance() function
rates<-prediction(preds, test_white$quality_cat, label.ordering = c("low", "high"))
##store the true positive and false positive rates
roc_result<-performance(rates,measure="tpr", x.measure="fpr")


##plot ROC curve and overlay the diagonal line for random guessing
plot(roc_result, main="ROC Curve for White Wine Quality Prediction")
lines(x = c(0,1), y = c(0,1), col="red")

auc<-performance(rates, measure = "auc")
auc@y.values
```

This ROC curve has a area of 0.77, which, while it doesn't always translate to a good prediction in our case, it does suggest this model predicts well.  


```{r}
preds_7<-predict(model7_white, newdata=test_white, type="response")
table(test_white$quality_cat, preds_7>0.5)

preds_full<-predict(full_white, newdata=test_white, type="response")
table(test_white$quality_cat, preds_full>0.5)

preds_8<-predict(model8_white, newdata=test_white, type="response")
table(test_white$quality_cat, preds_8>0.5)

preds_5<-predict(model5_white, newdata=test_white, type="response")
table(test_white$quality_cat, preds_5>0.5)
```

The initial model on 7 predictors has a true positive probability of $\frac{64}{64+43} \approx 0.598$  
The full model has a true positive probability of $\frac{63}{63+42} = 0.6$  
The model on 8 predictors has a true positive probability of $\frac{55}{55+36} \approx 0.604$  
Finally, the model on 5 predictors has a true positive probability of $\frac{56}{56+37} \approx 0.602$  

Overall, the best model is clearly the five predictor model, which achieves the nearly identical accuracy as the previous models, contains the fewest predictors, and has no multicolinearity.  

## Outliers or Transformations

Finally, we need to assess the rest of the regression assumptions.  

```{r}
probabilities <- predict(model5_white, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
head(predicted.classes)

#ASSUMPTION 1) Linearity assumption
# Select only numeric predictors
model_white <- train_white%>%
  dplyr::select(is.numeric) 
predictors <- colnames(model_white)
#Bind the logit and tidying the data for plot
model_white <- model_white %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)

ggplot(model_white, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")
```

None of these scatterplots show any of the predictors are clearly nonlinear, so there is no need to do any transformations.  

```{r}
res<-model5_white$residuals
#studentized residuals 
student.res<-rstandard(model5_white)
#externally studentized residuals
ext.student.res<-rstudent(model5_white)

res.frame<-data.frame(res,student.res,ext.student.res)

par(mfrow=c(1,2))
plot(model5_white$fitted.values,student.res,
     main="Studentized Residuals",
     ylim=c(-4.5,4.5))
plot(model5_white$fitted.values,ext.student.res,
     main="Externally Studentized Residuals",
     ylim=c(-4.5,4.5))

```

According to these plots, there doesn't seem to be any outliers in the residuals.






